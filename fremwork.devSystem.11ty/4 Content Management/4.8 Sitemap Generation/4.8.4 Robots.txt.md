|**Subtopic**|**Focus & Purpose**|**Key Concepts / Details**|**One-Line Recall**|
|---|---|---|---|
|**[[4.8.4 Robots.txt]]**|Control search engine access|Access control, crawler directives|Robots.txt tells crawlers what to access.|
|**[[4.8.4.1 Robots.txt Basics]]**|Understand robots.txt purpose|User-agent, Allow/Disallow directives|Robots.txt is a simple crawler instruction file.|
|**[[4.8.4.2 Creating robots.txt]]**|Generate robots.txt with Eleventy|Template file, passthrough copy|Create robots.txt as a template or static file.|
|**[[4.8.4.3 Sitemap Reference]]**|Link sitemap from robots.txt|`Sitemap:` directive, absolute URL|Point crawlers to your sitemap in robots.txt.|
|**[[4.8.4.4 Disallow Rules]]**|Block crawlers from paths|Disallow patterns, private areas|Disallow prevents crawling, not indexing.|
|**[[4.8.4.5 Crawl-Delay]]**|Manage crawler request rate|Server load, rate limiting|Crawl-delay reduces server load from crawlers.|
|**[[4.8.4.6 Testing robots.txt]]**|Validate crawler access rules|Google Search Console, testing tools|Test rules to ensure correct crawler behavior.|
